---
title: "HW 5"
output:
  word_document: default
  html_document: default
---

## R Markdown

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(TSA)
library(fpp)
library(tseries)
library(ggplot2)
library(forecast)
library(MLmetrics)
```


##1. Load, Split the data into train and tests sets
```{r split, echo =TRUE}
load("C:/Users/manya/Downloads/condmilk.rda")
traind<- window(condmilk, 1971, c(1979,12)) 
testd<- window(condmilk, c(1980,1), c(1980,12)) 


```

##2. Plot the training dataset.
```{r plot train, echo =TRUE}
autoplot(traind)
```

There does seem to be a slight decrease in variance over time so a slight Box-Cox transformation seems applicable. However, the Box-Cox transformation has been omitted, because the function is resulting in an empty data set for some reason (see below).



##2.b BoxCox Transformation
```{r BoxCox, echo =TRUE}

lambda<-BoxCox.lambda(traind)
traind<-BoxCox(traind, lambda)
#head(train)
```


##3. Determine stationarity
```{r stationarity, echo =TRUE}
adf.test(traind)
kpss.test(traind)

train_diff<-diff(traind)
plot(train_diff)
adf.test(train_diff)
kpss.test(train_diff)

train_diff2<-diff(train_diff)
plot(train_diff2)
#deseasonalized

acf(train_diff2)
pacf(train_diff2)

```

The ADF test for the training data results in a p-value small enough to reject the null. Also, the KPSS test for the 2nd order difference results in a p-value too large to reject the null. Thus it appears that the training dataset is stationary. Furthermore, when we examine the ACF of the twice-differenced data, we see that the plot does not decay immediately. This indicates that the data is stationary. 


##4. Fit ARIMA models
```{r arima, echo =TRUE}

autoarima<-auto.arima(traind)
autoarima

arimad1<-auto.arima(traind, d=1, D=1)
arimad1

```
For the autogenerated Arima model, the following values were assigned:
p=1, d=0, q=0, P=2, D=1, Q=0, AICc=410.37,   BIC=-400.55

For the Arima model where d and D were set to 1, the following values were assigned:
p=1, d=1, q=1, P=2, D=1, Q=0, AICc=-399.58,  BIC=-387.48 

The AICc and BIC values indicate the models have very similar predictive power, with a slight preference for the model where {d,D}= 1. 

##5. Plot Residuals ACFs
```{r acf, echo =TRUE}
print("AutoArima residuals")
checkresiduals(autoarima)

print("Arima, d=1, D=1 residuals")
checkresiduals(arimad1)

Box.test(autoarima$residuals, lag = 12, type = "Ljung-Box")
Box.test(arimad1$residuals, lag = 12, type = "Ljung-Box")
```
The p-values for both tests are too large to reject the null. This indicates that the model does not show a lack of fit, suggesting the models are valid. 

##6. Forecast and plot forecasts against test data
```{r forecast, echo =TRUE}

autoforecast<-forecast(autoarima, h=12)
d1forecast<-forecast(arimad1, h=12)

inv_auto<-InvBoxCox(autoforecast$mean, lambda, biasadj = FALSE, fvar = NULL)
inv_d1<-InvBoxCox(d1forecast$mean, lambda, biasadj = FALSE, fvar = NULL)

autoplot(testd) +
  autolayer(inv_auto, series="AutoArima") +
  autolayer(inv_d1, series="D1 Arima") +
  xlab("Year") + ylab("Sales")

```

##7. Compare forecasted values to actual test values
```{r compare_forecast, echo =TRUE}
accuracy(inv_auto, testd)
accuracy(inv_d1, testd)
```

The forecasted values from both models are very close in terms of their predictions. In truth, both models forecasted values closer to each other than to the true values, so some additional tweaking to the model seems advisable.  